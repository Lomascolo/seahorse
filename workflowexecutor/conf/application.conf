workflow-manager {
  workflows {
    path = "v1/workflows"
  }
  reports {
    path = "v1/reports"
  }
  timeout = 10
}

subscription-timeout = 15

executor-actor.init-timeout = 20

# Session Executor logs Keep-Alive messages to avoid being considered
# a stale process. This is interval between each logs, in seconds.
keep-alive.interval = 60

# In seconds
heartbeat.interval = 1

node-executor-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 10.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 50
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}

pyspark {
  # Directories that should be added to PYTHONPATH.
  # They will be prefixed with pyspark's path.
  python-path = [
    "lib/py4j-0.9-src.zip",
    "build"
  ]
}

pythoncaretaker {
  # Python executable used to start PyExecutor.
  python-binary = "/opt/conda/bin/python"
}

kernelmanagercaretaker {
  # Timeout (in seconds) for KernelManager to send back
  # the KernelManagerReady message
  timeout = 500

  # Script executed after Kernel Manager is unzipped.
  # This is a relative path (root in the temporary directory with unzipped KM).
  #startup-script = "executing_kernel/executing_kernel_manager.py"
  startup-script = "start.sh"
}
